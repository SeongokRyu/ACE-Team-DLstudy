{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, we will learn how to use weight decay methods to regularize our models. \n",
    "\n",
    "First, let's import tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to apply weight decay with tensorflow. \n",
    "For a practice, we will use a multi-layer perceptron for our model.\n",
    "\n",
    "### Method 1 : Using tf.get_regularization_loss()**\n",
    "\n",
    "* Documentation of tf.GraphKeys: https://www.tensorflow.org/api_docs/python/tf/GraphKeys\n",
    "\n",
    "\"tf.GraphKeys\" is the standard library uses various well-known names to collect and retrieve values associtated with a (computational) graph. To investigate the usage of tf.GraphKeys, let's define the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "y = tf.placeholder(tf.float32, shape=[None, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be found in the documentation of \"tf.layers.dense\", we can declare which weight (and bias) parameters to apply regularizations and regularization scale. \n",
    "\n",
    "* Documentation of tf.layers.dense: https://www.tensorflow.org/api_docs/python/tf/layers/dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 14:49:44.526269 139865723090688 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0801 14:49:44.528181 139865723090688 deprecation.py:323] From <ipython-input-3-61e56739eba4>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0801 14:49:44.530981 139865723090688 deprecation.py:506] From /home/wykgroup/appl/anaconda3/envs/ML_study/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "h = tf.layers.dense(x, \n",
    "                    units=64,\n",
    "                    use_bias=True,\n",
    "                    activation=tf.nn.relu,\n",
    "                    kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1e-3))\n",
    "y_pred = tf.layers.dense(h, \n",
    "                    units=1,\n",
    "                    use_bias=True,\n",
    "                    activation=None,\n",
    "                    kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1e-3))\n",
    "y_pred = tf.reshape(y_pred, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly configure whether to apply l2-(or l1-, l1l2-) regularization.\n",
    "As can be found in the below link, the scale corresponds to the \\\\(\\lambda \\) of \\( L_{total} = L_{nll} + \\lambda |w|^2 \\\\).\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/contrib/layers/python/layers/regularizers.py#L76-L109\n",
    "\n",
    "Then, we can use tf.GraphKeys to add the regularization loss term \\\\( \\lambda |w|^2 \\\\) in the total loss \\\\(L_{total} \\\\). The regularization loss term can be called by \"tf.losses.get_regularization_loss()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'total_regularization_loss:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.losses.get_regularization_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the total loss as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 14:49:44.788635 139865723090688 deprecation.py:323] From /home/wykgroup/appl/anaconda3/envs/ML_study/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "nll = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred))\n",
    "reg_loss = tf.losses.get_regularization_loss()\n",
    "total_loss = nll + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should minimize this \"total_loss\" to optimize our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Direcly add the product of scaling factor and norm of weight parameters\n",
    "    \n",
    "The second way is using tf.GraphKeys.TRAINABLE_VARIABLES to directly add the product of scaling factor and norm of weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(10, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the collection of trainable parameters can be called by the above command. \n",
    "To regulraize weight parameters, we can implement the code as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_loss = 0\n",
    "for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    if 'kernel' in v.name:\n",
    "        reg_loss += tf.nn.l2_loss(v)\n",
    "reg_loss *= 1e-3 # lambda, regularization scale        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of regularization loss is totally same as using \"tf.contrib.layers.l2_regularizer(scale=1e-3) and tf.losses.get_regularization_loss()\".\n",
    "\n",
    "### Method3: AdamW optimzier\n",
    "\n",
    "However, Ilya Loshchilov and Frank Hutter reported that weight decay with adaptive momentum optimizers (e.g. Adam optimizer) is actually not identical to L2-regularization and suggested new optimizers for more effective weight decay method, so-called the AdamW optimizer. \n",
    "\n",
    "* Documentation of the AdamW optimizer: https://www.tensorflow.org/api_docs/python/tf/contrib/opt/AdamWOptimizer\n",
    "* Reference: https://openreview.net/forum?id=Bkg6RiCqY7\n",
    "* Blog: https://www.fast.ai/2018/07/02/adam-weight-decay/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if 'kernel' in v.name ]\n",
    "adamw_optimizer = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-3, \n",
    "                                                learning_rate=1e-3)\n",
    "train_op = adamw_optimizer.minimize(nll, \n",
    "                                     decay_var_list=decay_var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, we learn a variety of methods to apply weight decay (L2-regularization) methods. \n",
    "Comprehensively, we recommend to use \"AdamW optimizer\" based on the results of previous literatures and to read the attatched documentations carefully. \n",
    "Especially, please understand how \"tf.GraphKeys\" works and usage of them, as examplified in this documents such as \"tf.GraphKeys.TRAINABLE_VARIABLES\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
